{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import io \n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::396608815783:role/service-role/AmazonSageMaker-ExecutionRole-20250207T102425 us-west-2\n"
     ]
    }
   ],
   "source": [
    "# iam_role = \"arn:aws:iam::1111111111:role/service-role/AmazonSageMaker-ExecutionRole-00000000T000000\"\n",
    "# sagemaker_session = sagemaker.session.Session()\n",
    "# region = sess._region_name\n",
    "# region = \"us-west-2\"\n",
    "# smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "sm_autoscaling_client = boto3.client(\"application-autoscaling\")\n",
    "\n",
    "print(role, region)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_uri = sagemaker.image_uris.retrieve(framework=\"djl-lmi\", version=\"0.28.0\", region=region)\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"phi3-4k-lmi-endpoint\")\n",
    "\n",
    "model = sagemaker.Model(\n",
    "    image_uri=container_uri, \n",
    "    role=role,\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "        \"TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"2\",\n",
    "        \"OPTION_DTYPE\":\"fp16\",\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    instance_type=instance_type,\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"phi3-4k-lmi-endpoint-2025-08-11-06-22-21-917\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a predictor for your endpoint\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"0.8.\\n\\nIn the context of Optimal Stopping Theory, what is the Kelly Criterion and how does it determine the size of bets to maximize wealth expectancy over time, given a probability of success (p) and a probability of failure (q)? To understand the Kelly Criterion within the framework of Optimal Stopping Theory, we need to delve into two concepts: Optimal Stopping Theory and the Kelly Criterion.\\n\\n### Optimal Stopping Theory\\n\\nOptimal Stopping Theory is a branch of probability theory and decision theory that deals with the problem of choosing a time to take a particular action, to maximize an expected reward or minimize a cost. The fundamental idea is that there's an optimal point in time where stopping and taking the action (or starting it) yields the best possible outcome based on available information.\\n\\n### Kelly Criterion\\n\\nThe Kelly Criterion is a formula used to determine the optimal size of a series of bets when the odds are known and the bettor's goal is wealth maximization over time. The criterion was developed by John L. Kelly, Jr. in 1956 and offers a strategy\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a prediction with your endpoint\n",
    "outputs = predictor.predict(\n",
    "    {\n",
    "        \"inputs\": \"The ai agent is \",\n",
    "        \"parameters\": {\"do_sample\": True, \"max_new_tokens\": 256},\n",
    "    }\n",
    ")\n",
    "\n",
    "outputs[\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input.\n",
    "\n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "\n",
    "    While usually each PayloadPart event from the event stream will contain a byte array\n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "\n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read\n",
    "    position to ensure that previous bytes are not exposed again.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if \"PayloadPart\" not in chunk:\n",
    "                print(\"Unknown event type:\" + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is a philosophical question that has puzzled humans for centuries. Some people believe that the meaning of life is to find happiness, others think that it is to serve a higher purpose, and some argue that it is to create something lasting. What is your perspective on the meaning of life?### Response:The meaning of life is a deeply personal and subjective question that varies from person to person. Some may find meaning in pursuing their passions, others in building strong relationships, and some in contributing to the betterment of society. Ultimately, the meaning of life is unique to each individual and can evolve over time."
     ]
    }
   ],
   "source": [
    "stop_token = \"\\n\"\n",
    "\n",
    "# Create body object and pass 'stream' to True\n",
    "body = {\n",
    "    \"inputs\": \"The meaning of life\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 400,\n",
    "        # \"return_full_text\": False  # This does not work with Phi3\n",
    "    },\n",
    "    \"stream\": True,\n",
    "}\n",
    "\n",
    "# Invoke the endpoint\n",
    "resp = sm_runtime_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name, Body=json.dumps(body), ContentType=\"application/json\"\n",
    ")\n",
    "\n",
    "# Parse the streaming response\n",
    "event_stream = resp[\"Body\"]\n",
    "start_json = b\"{\"\n",
    "for line in LineIterator(event_stream):\n",
    "    if line != b\"\" and start_json in line:\n",
    "        data = json.loads(line[line.find(start_json) :].decode(\"utf-8\"))\n",
    "        if data[\"token\"][\"text\"] != stop_token:\n",
    "            print(data[\"token\"][\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sm_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Delete model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43msm_client\u001b[49m.delete_model(ModelName=model_name)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Delete endpoint\u001b[39;00m\n\u001b[32m      5\u001b[39m sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
      "\u001b[31mNameError\u001b[39m: name 'sm_client' is not defined"
     ]
    }
   ],
   "source": [
    "# Delete model\n",
    "sm_client.delete_model(ModelName=model_name)\n",
    "\n",
    "# Delete endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
