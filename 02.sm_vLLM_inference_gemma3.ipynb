{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma3 through vLLM on Sagemaker\n",
    "\n",
    "## Gemma3 : SLM(Small Language Model) \n",
    "[특징]\n",
    "- 다양한 모델 크기 : 1B, 4B, 12B, 27B \n",
    "- 다국어 지원 : 140개 이상의 언어 지원, 특히 한국어 성능 우수\n",
    "- 멀티모달 기능 : 텍스트뿐만 아니라 이미지, 비디오 분석 가능\n",
    "- 확장된 컨텍스트 윈도우 : 최대 128K 토큰 처리 가능 \n",
    "- 경량화 및 효율성 : 단일 GPU/TPU 및 저사양 기기에서도 높은 성능 발휘\n",
    "\n",
    "\n",
    "## Use DJL with the SageMaker Python SDK\n",
    "- SageMaker Python SDK를 사용하면 Deep Java Library를 이용하여 Amazon SageMaker에서 모델을 호스팅할 수 있습니다.\n",
    "- Deep Java Library (DJL) Serving은 DJL이 제공하는 고성능 범용 독립형 모델 서빙 솔루션입니다. DJL Serving은 다양한 프레임워크로 학습된 모델을 로드하는 것을 지원합니다.\n",
    "- SageMaker Python SDK를 사용하면 DeepSpeed와 HuggingFace Accelerate와 같은 백엔드를 활용하여 DJL Serving으로 대규모 모델을 호스팅할 수 있습니다.\n",
    "- DJL Serving의 지원 버전에 대한 정보는 AWS 문서를 참조하십시오.\n",
    "- 최신 지원 버전을 사용하는 것을 권장합니다. 왜냐하면 그곳에 우리의 개발 노력이 집중되어 있기 때문입니다.\n",
    "- SageMaker Python SDK 사용에 대한 일반적인 정보는 SageMaker Python SDK 사용하기를 참조하십시오.\n",
    "> REF: [BLOG] Deploy LLM with vLLM on SageMaker in only 13 lines of code\n",
    "\n",
    "\n",
    "> DJLServing LMI 이미지 리스트 : https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "\n",
    "> HF 모델에 따라, Huggingface에서 사용 License를 요구하는 모델이 있습니다. Gemma3의 경우 라이선스를 요구 합니다. huggingface 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.2 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "autogluon-multimodal 1.2 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires nltk<3.9,>=3.4.5, but you have nltk 3.9.1 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.55.1 which is incompatible.\n",
      "autogluon-timeseries 1.2 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.55.1 which is incompatible.\n",
      "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "spacy 3.8.5 requires thinc<8.4.0,>=8.3.4, but you have thinc 8.3.2 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "%pip install -qU sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Depoly model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "sm_autoscaling_client = boto3.client(\"application-autoscaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최신 버전 (DJL Serving 0.33)을 사용하는 경우, `sagemaker.image_uris.retrieve()`가 늦게 반영될 수도 있다. \n",
    "\n",
    "이 경우, container_uri 자체를 사용 하도록 합니다.  \n",
    "\n",
    "ex) https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "\n",
    "> * 참조 : https://github.com/aws/deep-learning-containers/blob/master/available_images.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"Gemma-3\"\n",
    "model_id = \"google/gemma-3-12b-pt\"\n",
    "instance_type = \"ml.g5.24xlarge\"\n",
    "\n",
    "# 최신번전 : version 0.33 할려면, URI를 직접 사용하거나,  --> sagemaker upgrade 필요 : `uv pip install -U sagemaker`\n",
    "# container_uri = sagemaker.image_uris.retrieve(\n",
    "#     framework=\"djl-lmi\", version=\"0.30.0\", region=region\n",
    "# )\n",
    "# container_uri\n",
    "container_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\"\n",
    "# 참조 : https://github.com/aws/deep-learning-containers/blob/master/available_images.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container_uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\n",
      "container_startup_health_check_timeout: 900\n",
      "instance_type: ml.g5.24xlarge\n",
      "endpoint_name: Gemma-3-2025-08-13-02-12-55-046\n"
     ]
    }
   ],
   "source": [
    "container_startup_health_check_timeout = 900\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(base_model)\n",
    "\n",
    "print (f'container_uri: {container_uri}')\n",
    "print (f'container_startup_health_check_timeout: {container_startup_health_check_timeout}')\n",
    "print (f'instance_type: {instance_type}')\n",
    "print (f'endpoint_name: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat model with env variables\n",
    "\n",
    "- Target model: DeepSeek-Coder-V2-Light-Instruct\n",
    "- Backend for attention computation in vLLM\n",
    "- Available options:\n",
    "    - \"TORCH_SDPA\": use torch.nn.MultiheadAttention\n",
    "    - \"FLASH_ATTN\": use FlashAttention\n",
    "    - \"XFORMERS\": use XFormers\n",
    "    - \"ROCM_FLASH\": use ROCmFlashAttention\n",
    "    - \"FLASHINFER\": use flashinfer\n",
    "\n",
    "\n",
    "- '\"OPTION_DISABLE_FLASH_ATTN\": \"false\"' is for HF Accelerate with Seq-Scheduler\n",
    "- It will be ignored when using vLLM beckend\n",
    "\n",
    "> [DOC] DJL-Container and Model Configurations (info. about properties)\n",
    "\n",
    "> [DOC] Backend Specific Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\",\"\")\n",
    "deploy_env = {\n",
    "    \"HF_MODEL_ID\": model_id,\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"64\",\n",
    "    \"OPTION_DTYPE\":\"fp16\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"4096\",\n",
    "    # \"VLLM_ATTENTION_BACKEND\": \"XFORMERS\", # meta의 경우 XFORMERS, FlashAttention (default)\n",
    "    #\"OPTION_DISABLE_FLASH_ATTN\": \"false\", ## HF Accelerate with Seq-Scheduler\n",
    "    \"HF_TOKEN\": HF_TOKEN, # \"<your token>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.Model(\n",
    "    image_uri=container_uri,\n",
    "    role=role,\n",
    "    env=deploy_env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model on Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------!"
     ]
    }
   ],
   "source": [
    "model.deploy(\n",
    "    instance_type=instance_type,\n",
    "    initial_instance_count=2,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=container_startup_health_check_timeout,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint_name = \"Gemma-3-2025-08-12-13-47-53-107\"\n",
    "# endpoint_name = \"Gemma-3-2025-08-13-02-12-55-046\"\n",
    "endpoint_name = \"gemma3-s3-vllm-async-2025-08-14-02-10-03-005\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': '\\n\\nAI 에이전트는 특정 목표를 달성하기 위해 환경과 상호 작용하는 프로그램입니다. 이러한 에이전트는 데이터를 분석하고, 결정을 내리고, 작업을 수행하여 목표를 달성합니다. AI 에이전트는 게임, 추천 시스템, 챗봇 등 다양한 응용 분야에서 사용됩니다. 이들은 인간처럼 생각하고 행동할 수 있도록 설계되었으며, 학습과 적응을 통해 성능을 향상시킬 수 있습니다. 예를 들어, 자율 주행차는 주변 환경을 인식하고, 경로를 계획하고, 안전하게 운전하는 AI 에이전트입니다.\\n'}\n"
     ]
    }
   ],
   "source": [
    "# 호출 예시\n",
    "response = predictor.predict({\n",
    "    \"inputs\": \"AI Agent에 대해 100단어 내외로 설명해 주세요.\",\n",
    "    # \"inputs\": \"tell me aboiut the AI Agent.\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"stop\": [\"<|endoftext|>\"]  # Stop sequences 지정 가능\n",
    "    }\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_payload(chat):\n",
    "        \n",
    "    # JSON 페이로드 생성\n",
    "    body = {\n",
    "        \"messages\": chat,\n",
    "        \"max_tokens\": 512,\n",
    "        \"stream\": True,\n",
    "        \"ignore_eos\": False\n",
    "    }\n",
    "    \n",
    "    # JSON을 문자열로 변환하고 bytes로 인코딩\n",
    "    return json.dumps(body).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"너는 질의응답 챗봇입니다. 사용자의 질문의 의도를 파악하여 답변합니다. 답변은 한국어로 합니다\"},\n",
    "    {\"role\": \"user\", \"content\": \"AWS AIML Specialist 솔루션즈 아키텍트 역할에 대해 설명해줘\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response:\n",
      "----------------------------------------\n",
      "AWS AIML (Amazon Interactive Multimedia Learning) Specialist 솔루션즈 아키텍트는 AWS에서 **인터랙티브 학습 경험**을 구축하고 운영하기 위한 전문적인 역할을 담당합니다. 단순히 기술적인 구현에만 집중하는 것이 아니라, 학습 목표, 대상, 콘텐츠 특성 등을 고려하여 최적의 AWS 서비스를 조합하고 아키텍처를 설계하며, 관련 서비스를 통합하여 시너지 효과를 창출합니다. \n",
      "\n",
      "좀 더 자세히 살펴보면, 다음과 같은 핵심적인 역할과 책임들을 수행합니다.\n",
      "\n",
      "**1. 요구사항 분석 및 솔루션 설계:**\n",
      "\n",
      "* **학습 목표 및 대상 분석:** 고객사의 학습 목표, 학습 대상의 특성 (연령, 배경 지식, 기술 수준 등)을 정확히 파악합니다.\n",
      "* **콘텐츠 분석:** 학습 콘텐츠의 형태 (동영상, 텍스트, 퀴즈, 시뮬레이션 등) 및 특성을 분석하여 적합한 기술 스택을 결정합니다.\n",
      "* **AWS 서비스 최적 조합:**  AIML, Amazon Kendra, Amazon Lex, Amazon Polly, Amazon Transcribe, Amazon SageMaker, Amazon CloudFront, Amazon S3 등 다양한 AWS 서비스를 효과적으로 조합하여 최적의 학습 솔루션을 설계합니다.\n",
      "* **아키텍처 설계:** 확장성, 안정성, 보안, 비용 효율성을 고려하여 견고하고 유지보수가 용이한 아키텍처를 설계합니다.\n",
      "* **기술 문서 작성:** 설계 내용을 상세하게 문서화하여 개발팀 및 고객사와의 이해도를 높입니다.\n",
      "\n",
      "**2. 기술 구현 및 통합:**\n",
      "\n",
      "* **AIML 엔진 구축 및 설정:** AWS AIML 엔진을 설정하고 구성하여 고객사의 학습 콘텐츠에 적용합니다.\n",
      "* **챗봇 및 음성 인터페이스 구축:** Amazon Lex, Amazon Polly 등을 활용하여 챗봇 또는 음성 인터페이스를 구축하고, 학습 콘텐츠와 연동합니다.\n",
      "* **지식 검색 시스템 구축:** Amazon Kendra를 활용하여 학습 콘텐츠에 대한 지식 검색 기능을 구현합니다.\n",
      "* **콘텐츠 변환 및 최적화:**  동영상, 텍스트 등 다양한 형태의 학습 콘텐츠를 최적화하여 다양한 플랫폼 및 장치에서 원활하게 실행되도록 합니다.\n",
      "* **API 연동 및 통합:**  외부 시스템 (LMS, CRM 등)과의 API 연동을 통해 학습 경험을 통합하고 확장합니다.\n",
      "\n",
      "**3. 운영 및 유지보수:**\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "CPU times: user 483 ms, sys: 127 ms, total: 610 ms\n",
      "Wall time: 6.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Invoke the endpoint\n",
    "resp = sm_runtime_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name, \n",
    "    Body=generate_payload(chat),\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "print(\"Generated response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "buffer = \"\"\n",
    "string = \"\" \n",
    "for event in resp['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        buffer += chunk\n",
    "        try:\n",
    "            # Try to parse the buffer as JSON\n",
    "            data = json.loads(buffer)\n",
    "            if 'choices' in data:\n",
    "                print(data['choices'][0]['delta']['content'], end='', flush=True)\n",
    "                string += data['choices'][0]['delta']['content'] \n",
    "            buffer = \"\"  # Clear the buffer after successful parsing\n",
    "        except json.JSONDecodeError:\n",
    "            # If parsing fails, keep the buffer for the next iteration\n",
    "            pass\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoScaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "# SageMaker expects resource id to be provided with the following structure\n",
    "resource_id = f\"endpoint/{endpoint_name}/variant/{resp['ProductionVariants'][0]['VariantName']}\"\n",
    "\n",
    "# Scaling configuration\n",
    "scaling_config_response = sm_autoscaling_client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\", \n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Scaling Policy\n",
    "policy_name = f\"scaling-policy-{endpoint_name}\"\n",
    "scaling_policy_response = sm_autoscaling_client.put_scaling_policy(\n",
    "    PolicyName=policy_name,\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 5.0, # Target for avg invocations per minutes\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerVariantInvocationsPerInstance\",\n",
    "        },\n",
    "        \"ScaleInCooldown\": 600, # Duration in seconds until scale in\n",
    "        \"ScaleOutCooldown\": 60 # Duration in seconds between scale out\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_autoscaling_client.describe_scaling_policies(ServiceNamespace=\"sagemaker\")\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4, depth=4)\n",
    "for i in response[\"ScalingPolicies\"]:\n",
    "    pp.pprint(i[\"PolicyName\"])\n",
    "    print(\"\")\n",
    "    if(\"TargetTrackingScalingPolicyConfiguration\" in i):\n",
    "        pp.pprint(i[\"TargetTrackingScalingPolicyConfiguration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 코딩 태스크를 위한 프롬프트 리스트\n",
    "prompts = [\n",
    "    \"write a quick sort algorithm in python.\",\n",
    "    \"Write a Python function to implement a binary search algorithm.\",\n",
    "    \"Create a JavaScript function to flatten a nested array.\",\n",
    "    \"Implement a simple REST API using Flask in Python.\",\n",
    "    \"Write a SQL query to find the top 5 customers by total purchase amount.\",\n",
    "    \"Create a React component for a todo list with basic CRUD operations.\",\n",
    "    \"Implement a depth-first search algorithm for a graph in C++.\",\n",
    "    \"Write a bash script to find and delete files older than 30 days.\",\n",
    "    \"Create a Python class to represent a deck of cards with shuffle and deal methods.\",\n",
    "    \"Write a regular expression to validate email addresses.\",\n",
    "    \"Implement a basic CI/CD pipeline using GitHub Actions.\"\n",
    "]\n",
    "\n",
    "def generate_payload():\n",
    "    # 랜덤하게 프롬프트 선택\n",
    "    prompt = random.choice(prompts)\n",
    "    \n",
    "    # JSON 페이로드 생성\n",
    "    body = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 400,\n",
    "            # \"return_full_text\": False  # This does not work with Phi3\n",
    "        },\n",
    "        \"stream\": True,\n",
    "    }\n",
    "    \n",
    "    # JSON을 문자열로 변환하고 bytes로 인코딩\n",
    "    return json.dumps(body).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "request_duration = 250\n",
    "end_time = time.time() + request_duration\n",
    "print(f\"Endpoint will be tested for {request_duration} seconds\")\n",
    "\n",
    "while time.time() < end_time:\n",
    "    payload = generate_payload()\n",
    "    # Invoke the endpoint\n",
    "    response = sm_runtime_client.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name, \n",
    "        # Body=json.dumps(body), \n",
    "        Body = payload,\n",
    "        ContentType=\"application/json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the instance counts after the endpoint gets more load\n",
    "response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "endpoint_status = response[\"EndpointStatus\"]\n",
    "request_duration = 250\n",
    "end_time = time.time() + request_duration\n",
    "print(f\"Waiting for Instance count increase for a max of {request_duration} seconds. Please re run this cell in case the count does not change\")\n",
    "while time.time() < end_time:\n",
    "    response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint_status = response[\"EndpointStatus\"]\n",
    "    instance_count = response[\"ProductionVariants\"][0][\"CurrentInstanceCount\"]\n",
    "    print(f\"Status: {endpoint_status}\")\n",
    "    print(f\"Current Instance count: {instance_count}\")\n",
    "    if (endpoint_status==\"InService\") and (instance_count>1):\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delete model\n",
    "sm_client.delete_model(ModelName=model_name)\n",
    "\n",
    "# Delete endpoint configuration\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "# Delete endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
