{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QWen 2.5 through vLLM on Sagemaker\n",
    "\n",
    "## Use DJL with the SageMaker Python SDK\n",
    "- SageMaker Python SDK를 사용하면 Deep Java Library를 이용하여 Amazon SageMaker에서 모델을 호스팅할 수 있습니다.\n",
    "- Deep Java Library (DJL) Serving은 DJL이 제공하는 고성능 범용 독립형 모델 서빙 솔루션입니다. DJL Serving은 다양한 프레임워크로 학습된 모델을 로드하는 것을 지원합니다.\n",
    "- SageMaker Python SDK를 사용하면 DeepSpeed와 HuggingFace Accelerate와 같은 백엔드를 활용하여 DJL Serving으로 대규모 모델을 호스팅할 수 있습니다.\n",
    "- DJL Serving의 지원 버전에 대한 정보는 AWS 문서를 참조하십시오.\n",
    "- 최신 지원 버전을 사용하는 것을 권장합니다. 왜냐하면 그곳에 우리의 개발 노력이 집중되어 있기 때문입니다.\n",
    "- SageMaker Python SDK 사용에 대한 일반적인 정보는 SageMaker Python SDK 사용하기를 참조하십시오.\n",
    "> REF: [BLOG] Deploy LLM with vLLM on SageMaker in only 13 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Depoly model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "sm_autoscaling_client = boto3.client(\"application-autoscaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 0.28.0. Ignoring framework/algorithm version: 0.30.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "instance_type = \"ml.g5.24xlarge\"\n",
    "\n",
    "container_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"djl-lmi\", version=\"0.30.0\", region=region\n",
    ")\n",
    "container_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container_uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124\n",
      "container_startup_health_check_timeout: 900\n",
      "instance_type: ml.g5.24xlarge\n",
      "endpoint_name: QWEN-2-5-7B-G5-2025-08-13-13-56-33-717\n"
     ]
    }
   ],
   "source": [
    "container_startup_health_check_timeout = 900\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"QWEN-2-5-7B-G5\")\n",
    "\n",
    "print (f'container_uri: {container_uri}')\n",
    "print (f'container_startup_health_check_timeout: {container_startup_health_check_timeout}')\n",
    "print (f'instance_type: {instance_type}')\n",
    "print (f'endpoint_name: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat model with env variables\n",
    "\n",
    "- Target model: DeepSeek-Coder-V2-Light-Instruct\n",
    "- Backend for attention computation in vLLM\n",
    "- Available options:\n",
    "    - \"TORCH_SDPA\": use torch.nn.MultiheadAttention\n",
    "    - \"FLASH_ATTN\": use FlashAttention\n",
    "    - \"XFORMERS\": use XFormers\n",
    "    - \"ROCM_FLASH\": use ROCmFlashAttention\n",
    "    - \"FLASHINFER\": use flashinfer\n",
    "\n",
    "\n",
    "- '\"OPTION_DISABLE_FLASH_ATTN\": \"false\"' is for HF Accelerate with Seq-Scheduler\n",
    "- It will be ignored when using vLLM beckend\n",
    "\n",
    "> [DOC] DJL-Container and Model Configurations (info. about properties)\n",
    "\n",
    "> [DOC] Backend Specific Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\",\"\")\n",
    "deploy_env = {\n",
    "    \"HF_MODEL_ID\": model_id,\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"64\",\n",
    "    \"OPTION_DTYPE\":\"fp16\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"4096\",\n",
    "    \"VLLM_ATTENTION_BACKEND\": \"XFORMERS\",\n",
    "    #\"OPTION_DISABLE_FLASH_ATTN\": \"false\", ## HF Accelerate with Seq-Scheduler\n",
    "    \"HF_TOKEN\": HF_TOKEN, # \"<your token>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.Model(\n",
    "    image_uri=container_uri,\n",
    "    role=role,\n",
    "    env=deploy_env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model on Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "model.deploy(\n",
    "    instance_type=instance_type,\n",
    "    initial_instance_count=2,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=container_startup_health_check_timeout,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"QWEN-2-5-7B-G5-2025-08-12-03-05-19-368\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': ' AI Agent는 인공지능 기술을 활용해 인간의 지시에 따라 작업을 수행하거나 의사결정을 돕는 컴퓨터 프로그램입니다. 다양한 분야에서 활용되며, 자연어 처리, 학습 알고리즘 등을 통해 지속적으로 발전하고 있습니다.'}\n"
     ]
    }
   ],
   "source": [
    "# 호출 예시\n",
    "response = predictor.predict({\n",
    "    \"inputs\": \"AI Agent에 대해 100단어 내외로 설명해 주세요.\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"stop\": [\"<|endoftext|>\"]  # Stop sequences 지정 가능\n",
    "    }\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_payload(chat):\n",
    "        \n",
    "    # JSON 페이로드 생성\n",
    "    body = {\n",
    "        \"messages\": chat,\n",
    "        \"max_tokens\": 512,\n",
    "        \"stream\": True,\n",
    "        \"ignore_eos\": False\n",
    "    }\n",
    "    \n",
    "    # JSON을 문자열로 변환하고 bytes로 인코딩\n",
    "    return json.dumps(body).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"너는 질의응답 챗봇입니다. 사용자의 질문의 의도를 파악하여 답변합니다. 답변은 한국어로 합니다\"},\n",
    "    {\"role\": \"user\", \"content\": \"AWS AIML Specialist 솔루션즈 아키텍트 역할에 대해 설명해줘\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response:\n",
      "----------------------------------------\n",
      "AWS AIML Specialist Soln Architect(AIML은 Artificial Intelligence and Machine Learning의 약자)는 AWS에서 제공하는 인공지능 및 machine learning 서비스를 사용하여 클라이언트가 데이터를 분석하고 비즈니스 문제를 해결할 수 있도록 하는 아키텍처 설계자 역할을 의미합니다. 이를 위해 다음의 역할이 요구됩니다:\n",
      "\n",
      "1. 데이터 분석 및 모델 개발: 비교적 큰 데이터셋에 대해 분석이 가능하고, 최적의 모델을 선택하여 개발할 수 있어야 합니다.\n",
      "\n",
      "2. 서비스 활용: AWS의 AIML 서비스를 잘 이해하고 활용할 수 있어야 합니다. AWS에서는 SageMaker, Comprehend, Forecast 등의 다양한 AIML 서비스를 제공하고 있습니다.\n",
      "\n",
      "3. 비즈니스ضع함: 클라이언트의 비즈니스 문제를 이해하고, 이를 해결할 수 있는 기술iske 솔루션을 제안하고 설계할 수 있어야 합니다.\n",
      "\n",
      "4. 커뮤니케이션: 클라이언트와의 소통을 원활히 하고, 기술적인 내용을 간단하게 설명하여 이해할 수 있어야 합니다.\n",
      "\n",
      "5. 아키텍처 설계: 적절한 데이터 처리, 저장 및 모델 구성을 위한 아키텍처를 설계할 수 있어야 합니다.\n",
      "\n",
      "6. 보안 및 품질 관리: AIML 모델의 안전性和质量都需要考虑。\n",
      "\n",
      "这种角色需要的技能和经验包括机器学习、人工智能、云架构设计以及与客户沟通的能力。同时，还需要熟悉AWS的服务和工具，以及在实际项目中应用这些技术的能力。\n",
      "----------------------------------------\n",
      "CPU times: user 204 ms, sys: 29.2 ms, total: 234 ms\n",
      "Wall time: 4.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Invoke the endpoint\n",
    "resp = sm_runtime_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name, \n",
    "    Body=generate_payload(chat),\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "print(\"Generated response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "buffer = \"\"\n",
    "string = \"\" \n",
    "for event in resp['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        buffer += chunk\n",
    "        try:\n",
    "            # Try to parse the buffer as JSON\n",
    "            data = json.loads(buffer)\n",
    "            if 'choices' in data:\n",
    "                print(data['choices'][0]['delta']['content'], end='', flush=True)\n",
    "                string += data['choices'][0]['delta']['content'] \n",
    "            buffer = \"\"  # Clear the buffer after successful parsing\n",
    "        except json.JSONDecodeError:\n",
    "            # If parsing fails, keep the buffer for the next iteration\n",
    "            pass\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoScaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "# SageMaker expects resource id to be provided with the following structure\n",
    "resource_id = f\"endpoint/{endpoint_name}/variant/{resp['ProductionVariants'][0]['VariantName']}\"\n",
    "\n",
    "# Scaling configuration\n",
    "scaling_config_response = sm_autoscaling_client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\", \n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp['ProductionVariants'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Scaling Policy\n",
    "policy_name = f\"scaling-policy-{endpoint_name}\"\n",
    "scaling_policy_response = sm_autoscaling_client.put_scaling_policy(\n",
    "    PolicyName=policy_name,\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 5.0, # Target for avg invocations per minutes\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerVariantInvocationsPerInstance\",\n",
    "        },\n",
    "        \"ScaleInCooldown\": 600, # Duration in seconds until scale in\n",
    "        \"ScaleOutCooldown\": 60 # Duration in seconds between scale out\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_autoscaling_client.describe_scaling_policies(ServiceNamespace=\"sagemaker\")\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4, depth=4)\n",
    "for i in response[\"ScalingPolicies\"]:\n",
    "    pp.pprint(i[\"PolicyName\"])\n",
    "    print(\"\")\n",
    "    if(\"TargetTrackingScalingPolicyConfiguration\" in i):\n",
    "        pp.pprint(i[\"TargetTrackingScalingPolicyConfiguration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 코딩 태스크를 위한 프롬프트 리스트\n",
    "prompts = [\n",
    "    \"write a quick sort algorithm in python.\",\n",
    "    \"Write a Python function to implement a binary search algorithm.\",\n",
    "    \"Create a JavaScript function to flatten a nested array.\",\n",
    "    \"Implement a simple REST API using Flask in Python.\",\n",
    "    \"Write a SQL query to find the top 5 customers by total purchase amount.\",\n",
    "    \"Create a React component for a todo list with basic CRUD operations.\",\n",
    "    \"Implement a depth-first search algorithm for a graph in C++.\",\n",
    "    \"Write a bash script to find and delete files older than 30 days.\",\n",
    "    \"Create a Python class to represent a deck of cards with shuffle and deal methods.\",\n",
    "    \"Write a regular expression to validate email addresses.\",\n",
    "    \"Implement a basic CI/CD pipeline using GitHub Actions.\"\n",
    "]\n",
    "\n",
    "def generate_payload():\n",
    "    # 랜덤하게 프롬프트 선택\n",
    "    prompt = random.choice(prompts)\n",
    "    \n",
    "    # JSON 페이로드 생성\n",
    "    body = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 400,\n",
    "            # \"return_full_text\": False  # This does not work with Phi3\n",
    "        },\n",
    "        \"stream\": True,\n",
    "    }\n",
    "    \n",
    "    # JSON을 문자열로 변환하고 bytes로 인코딩\n",
    "    return json.dumps(body).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "request_duration = 250\n",
    "end_time = time.time() + request_duration\n",
    "print(f\"Endpoint will be tested for {request_duration} seconds\")\n",
    "\n",
    "while time.time() < end_time:\n",
    "    payload = generate_payload()\n",
    "    # Invoke the endpoint\n",
    "    response = sm_runtime_client.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name, \n",
    "        # Body=json.dumps(body), \n",
    "        Body = payload,\n",
    "        ContentType=\"application/json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the instance counts after the endpoint gets more load\n",
    "response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "endpoint_status = response[\"EndpointStatus\"]\n",
    "request_duration = 250\n",
    "end_time = time.time() + request_duration\n",
    "print(f\"Waiting for Instance count increase for a max of {request_duration} seconds. Please re run this cell in case the count does not change\")\n",
    "while time.time() < end_time:\n",
    "    response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint_status = response[\"EndpointStatus\"]\n",
    "    instance_count = response[\"ProductionVariants\"][0][\"CurrentInstanceCount\"]\n",
    "    print(f\"Status: {endpoint_status}\")\n",
    "    print(f\"Current Instance count: {instance_count}\")\n",
    "    if (endpoint_status==\"InService\") and (instance_count>1):\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delete model\n",
    "sm_client.delete_model(ModelName=model_name)\n",
    "\n",
    "# Delete endpoint configuration\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "# Delete endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
