{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma3 through vLLM on Sagemaker\n",
    "\n",
    "## Gemma3 : SLM(Small Language Model) \n",
    "[특징]\n",
    "- 다양한 모델 크기 : 1B, 4B, 12B, 27B \n",
    "- 다국어 지원 : 140개 이상의 언어 지원, 특히 한국어 성능 우수\n",
    "- 멀티모달 기능 : 텍스트뿐만 아니라 이미지, 비디오 분석 가능\n",
    "- 확장된 컨텍스트 윈도우 : 최대 128K 토큰 처리 가능 \n",
    "- 경량화 및 효율성 : 단일 GPU/TPU 및 저사양 기기에서도 높은 성능 발휘\n",
    "\n",
    "\n",
    "## Use DJL with the SageMaker Python SDK\n",
    "- SageMaker Python SDK를 사용하면 Deep Java Library를 이용하여 Amazon SageMaker에서 모델을 호스팅할 수 있습니다.\n",
    "- Deep Java Library (DJL) Serving은 DJL이 제공하는 고성능 범용 독립형 모델 서빙 솔루션입니다. DJL Serving은 다양한 프레임워크로 학습된 모델을 로드하는 것을 지원합니다.\n",
    "- SageMaker Python SDK를 사용하면 DeepSpeed와 HuggingFace Accelerate와 같은 백엔드를 활용하여 DJL Serving으로 대규모 모델을 호스팅할 수 있습니다.\n",
    "- DJL Serving의 지원 버전에 대한 정보는 AWS 문서를 참조하십시오.\n",
    "- 최신 지원 버전을 사용하는 것을 권장합니다. 왜냐하면 그곳에 우리의 개발 노력이 집중되어 있기 때문입니다.\n",
    "- SageMaker Python SDK 사용에 대한 일반적인 정보는 SageMaker Python SDK 사용하기를 참조하십시오.\n",
    "> REF: [BLOG] Deploy LLM with vLLM on SageMaker in only 13 lines of code\n",
    "\n",
    "\n",
    "> DJLServing LMI 이미지 리스트 : https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "\n",
    "> HF 모델에 따라, Huggingface에서 사용 License를 요구하는 모델이 있습니다. Gemma3의 경우 라이선스를 요구 합니다. huggingface 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "%pip install -qU sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Depoly model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "sm_autoscaling_client = boto3.client(\"application-autoscaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최신 버전 (DJL Serving 0.33)을 사용하는 경우, `sagemaker.image_uris.retrieve()`가 늦게 반영될 수도 있다. \n",
    "\n",
    "이 경우, container_uri 자체를 사용 하도록 합니다.  \n",
    "\n",
    "ex) https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "\n",
    "> * 참조 : https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "\n",
    "[유의 사항]\n",
    "- S3에 저장하여 사용하는 모델 데이터는 공식적인 가이드는 \"압축 해제\" 형태 이다. \n",
    "- 저장된 S3는 `model_data`에 입력하며, `model_data`는 prefix까지의 uri이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S3DataSource': {'S3Uri': 's3://my-model-train/hf_models/gemma-3-4b-it-uncompressed/', 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}\n"
     ]
    }
   ],
   "source": [
    "base_model = \"Gemma-3\"\n",
    "# model_id = \"google/gemma-3-12b-it\"\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "instance_type = \"ml.g5.24xlarge\"\n",
    "\n",
    "\n",
    "bucket_uri = \"s3://my-model-train/hf_models\"\n",
    "# model_data = f\"{bucket_uri}/{model_id.split('/')[-1]}/model.tar.gz\"\n",
    "# S3 Prefix + 압축 해제(CompressionType=None)로 모델 마운트\n",
    "model_data = {\n",
    "    \"S3DataSource\": {\n",
    "        \"S3Uri\": \"s3://my-model-train/hf_models/gemma-3-4b-it-uncompressed/\", \n",
    "        \"S3DataType\": \"S3Prefix\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    }\n",
    "}\n",
    "print(model_data)\n",
    "\n",
    "# 최신번전 : version 0.33 할려면, URI를 직접 사용하거나,  --> sagemaker upgrade 필요 : `uv pip install -U sagemaker`\n",
    "# container_uri = sagemaker.image_uris.retrieve(\n",
    "#     framework=\"djl-lmi\", version=\"0.30.0\", region=region\n",
    "# )\n",
    "# container_uri\n",
    "container_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\"\n",
    "# 참조 : https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container_uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\n",
      "container_startup_health_check_timeout: 900\n",
      "instance_type: ml.g5.24xlarge\n",
      "endpoint_name: Gemma-3-2025-08-13-09-26-48-970\n"
     ]
    }
   ],
   "source": [
    "container_startup_health_check_timeout = 900\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(base_model)\n",
    "\n",
    "print (f'container_uri: {container_uri}')\n",
    "print (f'container_startup_health_check_timeout: {container_startup_health_check_timeout}')\n",
    "print (f'instance_type: {instance_type}')\n",
    "print (f'endpoint_name: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat model with env variables\n",
    "\n",
    "- Target model: DeepSeek-Coder-V2-Light-Instruct\n",
    "- Backend for attention computation in vLLM\n",
    "- Available options:\n",
    "    - \"TORCH_SDPA\": use torch.nn.MultiheadAttention\n",
    "    - \"FLASH_ATTN\": use FlashAttention\n",
    "    - \"XFORMERS\": use XFormers\n",
    "    - \"ROCM_FLASH\": use ROCmFlashAttention\n",
    "    - \"FLASHINFER\": use flashinfer\n",
    "\n",
    "\n",
    "- '\"OPTION_DISABLE_FLASH_ATTN\": \"false\"' is for HF Accelerate with Seq-Scheduler\n",
    "- It will be ignored when using vLLM beckend\n",
    "\n",
    "> [DOC] DJL-Container and Model Configurations (info. about properties)\n",
    "\n",
    "> [DOC] Backend Specific Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\",\"\")\n",
    "\n",
    "deploy_env = {\n",
    "    # \"HF_MODEL_ID\": model_id,\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"8\",\n",
    "    # \"OPTION_DTYPE\":\"fp16\",\n",
    "    \"OPTION_DTYPE\":\"bf16\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "    \"OPTION_MODEL_IMPL\": \"transformers\",\n",
    "    # \"OPTION_MAX_MODEL_LEN\": \"4096\",\n",
    "    # \"VLLM_ATTENTION_BACKEND\": \"XFORMERS\", # meta의 경우 XFORMERS, FlashAttention (default)\n",
    "    #\"OPTION_DISABLE_FLASH_ATTN\": \"false\", ## HF Accelerate with Seq-Scheduler\n",
    "    \"HF_TOKEN\": HF_TOKEN, # \"<your token>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.Model(\n",
    "    image_uri=container_uri,\n",
    "    model_data=model_data,  # from S3 bucket - model.tar.gz\n",
    "    role=role,\n",
    "    env=deploy_env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model on Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    instance_type=instance_type,\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=container_startup_health_check_timeout,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint_name = \"Gemma-3-2025-08-12-13-47-53-107\"\n",
    "# endpoint_name = \"Gemma-3-2025-08-13-02-12-55-046\"\n",
    "endpoint_name = \"gemma3-s3-vllm-async-2025-08-14-02-10-03-005\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': '\\n\\nAI 에이전트는 인간과 유사한 지능을 가진 컴퓨터 프로그램입니다. 주어진 목표를 달성하기 위해 정보를 수집, 분석 및 결정을 내리는 과정을 자동화합니다. 그들은 자연어 처리, 기계 학습 및 컴퓨터 비전과 같은 다양한 AI 기술을 사용하여 주변 세계를 이해하고 이에 적응합니다. AI 에이전트는 챗봇, 자율 주행차 및 개인 비서와 같은 다양한 응용 분야에 사용될 수 있습니다. 이들은 인간의 개입 없이 복잡한 작업을 수행할 수 있는 능력으로 인해 점점 더 중요해지고 있습니다.\\n'}\n"
     ]
    }
   ],
   "source": [
    "# 호출 예시\n",
    "response = predictor.predict({\n",
    "    \"inputs\": \"AI Agent에 대해 100단어 내외로 설명해 주세요.\",\n",
    "    # \"inputs\": \"tell me aboiut the AI Agent.\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"stop\": [\"<|endoftext|>\"]  # Stop sequences 지정 가능\n",
    "    }\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_payload(chat):\n",
    "        \n",
    "    # JSON 페이로드 생성\n",
    "    body = {\n",
    "        \"messages\": chat,\n",
    "        \"max_tokens\": 512,\n",
    "        \"stream\": True,\n",
    "        \"ignore_eos\": False\n",
    "    }\n",
    "    \n",
    "    # JSON을 문자열로 변환하고 bytes로 인코딩\n",
    "    return json.dumps(body).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"너는 질의응답 챗봇입니다. 사용자의 질문의 의도를 파악하여 답변합니다. 답변은 한국어로 합니다\"},\n",
    "    {\"role\": \"user\", \"content\": \"AWS AIML Specialist 솔루션즈 아키텍트 역할에 대해 설명해줘\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response:\n",
      "----------------------------------------\n",
      "AWS AIML Specialist 솔루션즈 아키텍트는 AWS에서 AI 및 머신러닝 솔루션을 설계, 구축, 배포, 운영하는 데 특화된 전문가입니다. 단순히 기술적인 지식을 넘어, 고객의 비즈니스 목표를 이해하고 AI/ML 기술을 통해 이를 달성할 수 있는 최적의 솔루션을 제시하는 역할을 수행합니다. \n",
      "\n",
      "좀 더 자세히 설명하자면 다음과 같은 특징과 책임을 갖습니다.\n",
      "\n",
      "**1. 전문 지식 및 기술 역량:**\n",
      "\n",
      "* **AWS AI/ML 서비스:** Amazon SageMaker, Amazon Lex, Amazon Polly, Amazon Rekognition, Amazon Comprehend, Amazon Transcribe 등 다양한 AWS AI/ML 서비스를 깊이 있게 이해하고 활용할 수 있습니다.\n",
      "* **머신러닝 모델 개발 및 배포:** 모델 학습, 평가, 배포 및 관리 전반에 대한 전문성을 보유하고 있으며, 모델의 성능 최적화, 비용 효율성 개선 등에 대한 경험이 있습니다.\n",
      "* **데이터 엔지니어링:** 대용량 데이터 처리, 데이터 파이프라인 구축, 데이터 품질 관리 등 데이터 엔지니어링 역량을 갖추고 있습니다.\n",
      "* **아키텍처 설계:** 확장성, 안정성, 보안, 비용 효율성을 고려하여 최적의 AI/ML 아키텍처를 설계할 수 있습니다.\n",
      "* **DevOps:** CI/CD, Infrastructure as Code (IaC) 등 DevOps practices를 활용하여 AI/ML 솔루션의 개발 및 배포 프로세스를 자동화하고 효율성을 높일 수 있습니다.\n",
      "\n",
      "**2. 고객과의 협업 및 비즈니스 이해:**\n",
      "\n",
      "* **비즈니스 요구사항 분석:** 고객의 비즈니스 목표, 문제점, 요구사항을 정확하게 파악하고 이해합니다.\n",
      "* **솔루션 제안:** AI/ML 기술을 활용하여 고객의 비즈니스 문제를 해결하고 목표를 달성할 수 있는 맞춤형 솔루션을 제안합니다.\n",
      "* **성공적인 솔루션 구현:** 고객과 긴밀하게 협력하여 AI/ML 솔루션을 성공적으로 구현하고, 고객의 기대치를 충족시키도록 지원합니다.\n",
      "* **ROI (투자 수익률) 분석:** AI/ML 솔루션의 ROI를 분석하고, 고객에게 명확한 가치를 제공할 수 있도록 합니다.\n",
      "\n",
      "**3. 책임 및 업무 범위:**\n",
      "\n",
      "* **솔루션 설계:** 고객의 요구사항\n",
      "----------------------------------------\n",
      "CPU times: user 477 ms, sys: 148 ms, total: 625 ms\n",
      "Wall time: 6.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Invoke the endpoint\n",
    "resp = sm_runtime_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name, \n",
    "    Body=generate_payload(chat),\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "print(\"Generated response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "buffer = \"\"\n",
    "string = \"\" \n",
    "for event in resp['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        buffer += chunk\n",
    "        try:\n",
    "            # Try to parse the buffer as JSON\n",
    "            data = json.loads(buffer)\n",
    "            if 'choices' in data:\n",
    "                print(data['choices'][0]['delta']['content'], end='', flush=True)\n",
    "                string += data['choices'][0]['delta']['content'] \n",
    "            buffer = \"\"  # Clear the buffer after successful parsing\n",
    "        except json.JSONDecodeError:\n",
    "            # If parsing fails, keep the buffer for the next iteration\n",
    "            pass\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoScaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "# SageMaker expects resource id to be provided with the following structure\n",
    "resource_id = f\"endpoint/{endpoint_name}/variant/{resp['ProductionVariants'][0]['VariantName']}\"\n",
    "\n",
    "# Scaling configuration\n",
    "scaling_config_response = sm_autoscaling_client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\", \n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Scaling Policy\n",
    "policy_name = f\"scaling-policy-{endpoint_name}\"\n",
    "scaling_policy_response = sm_autoscaling_client.put_scaling_policy(\n",
    "    PolicyName=policy_name,\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 5.0, # Target for avg invocations per minutes\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerVariantInvocationsPerInstance\",\n",
    "        },\n",
    "        \"ScaleInCooldown\": 600, # Duration in seconds until scale in\n",
    "        \"ScaleOutCooldown\": 60 # Duration in seconds between scale out\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_autoscaling_client.describe_scaling_policies(ServiceNamespace=\"sagemaker\")\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4, depth=4)\n",
    "for i in response[\"ScalingPolicies\"]:\n",
    "    pp.pprint(i[\"PolicyName\"])\n",
    "    print(\"\")\n",
    "    if(\"TargetTrackingScalingPolicyConfiguration\" in i):\n",
    "        pp.pprint(i[\"TargetTrackingScalingPolicyConfiguration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 코딩 태스크를 위한 프롬프트 리스트\n",
    "prompts = [\n",
    "    \"write a quick sort algorithm in python.\",\n",
    "    \"Write a Python function to implement a binary search algorithm.\",\n",
    "    \"Create a JavaScript function to flatten a nested array.\",\n",
    "    \"Implement a simple REST API using Flask in Python.\",\n",
    "    \"Write a SQL query to find the top 5 customers by total purchase amount.\",\n",
    "    \"Create a React component for a todo list with basic CRUD operations.\",\n",
    "    \"Implement a depth-first search algorithm for a graph in C++.\",\n",
    "    \"Write a bash script to find and delete files older than 30 days.\",\n",
    "    \"Create a Python class to represent a deck of cards with shuffle and deal methods.\",\n",
    "    \"Write a regular expression to validate email addresses.\",\n",
    "    \"Implement a basic CI/CD pipeline using GitHub Actions.\"\n",
    "]\n",
    "\n",
    "def generate_payload():\n",
    "    # 랜덤하게 프롬프트 선택\n",
    "    prompt = random.choice(prompts)\n",
    "    \n",
    "    # JSON 페이로드 생성\n",
    "    body = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 400,\n",
    "            # \"return_full_text\": False  # This does not work with Phi3\n",
    "        },\n",
    "        \"stream\": True,\n",
    "    }\n",
    "    \n",
    "    # JSON을 문자열로 변환하고 bytes로 인코딩\n",
    "    return json.dumps(body).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "request_duration = 250\n",
    "end_time = time.time() + request_duration\n",
    "print(f\"Endpoint will be tested for {request_duration} seconds\")\n",
    "\n",
    "while time.time() < end_time:\n",
    "    payload = generate_payload()\n",
    "    # Invoke the endpoint\n",
    "    response = sm_runtime_client.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name, \n",
    "        # Body=json.dumps(body), \n",
    "        Body = payload,\n",
    "        ContentType=\"application/json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the instance counts after the endpoint gets more load\n",
    "response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "endpoint_status = response[\"EndpointStatus\"]\n",
    "request_duration = 250\n",
    "end_time = time.time() + request_duration\n",
    "print(f\"Waiting for Instance count increase for a max of {request_duration} seconds. Please re run this cell in case the count does not change\")\n",
    "while time.time() < end_time:\n",
    "    response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint_status = response[\"EndpointStatus\"]\n",
    "    instance_count = response[\"ProductionVariants\"][0][\"CurrentInstanceCount\"]\n",
    "    print(f\"Status: {endpoint_status}\")\n",
    "    print(f\"Current Instance count: {instance_count}\")\n",
    "    if (endpoint_status==\"InService\") and (instance_count>1):\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delete model\n",
    "sm_client.delete_model(ModelName=model_name)\n",
    "\n",
    "# Delete endpoint configuration\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "# Delete endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
